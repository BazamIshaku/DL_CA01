# -*- coding: utf-8 -*-
"""20061319_DL_CA01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRaxVfhGgnXsWg3YO36wy9nEwRqagxqA
"""
#
import json
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pycocotools.cocoeval import COCOeval

from google.colab import files
uploaded = files.upload()

import zipfile
import os

zip_path = '/content/CA01 Project.zip'  # Colab stores your upload here
extract_dir = '/content/CA01_Project'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# Check the directory structure
os.listdir(extract_dir)

annotation_file = '/content/CA01_Project/CA01 Project/lvis_v1_train.json/_annotations.coco.json'
images_dir = '/content/CA01_Project/CA01 Project/train2017'

from pycocotools.coco import COCO

annotation_file = '/content/CA01_Project/CA01 Project/lvis_v1_train.json/_annotations.coco.json'
coco = COCO(annotation_file)

# List all categories in your dataset
cat_ids = coco.getCatIds()
cats = coco.loadCats(cat_ids)
category_names = [cat['name'] for cat in cats]
print('Categories:', category_names)

# Confirm how many images you have
img_ids = coco.getImgIds()
print('Number of images:', len(img_ids))

from transformers import OwlViTProcessor, OwlViTForObjectDetection
from PIL import Image
import torch
import os

# Load pretrained OWL-ViT model
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")

# List all categories in your dataset
cat_ids = coco.getCatIds()
cats = coco.loadCats(cat_ids)
category_names = [cat['name'] for cat in cats]
print('Categories:', category_names)

# Confirm how many images you have
img_ids = coco.getImgIds()
print('Number of images:', len(img_ids))

CATEGORY_MAPPING = {
    # Sports-related activities
    "man doing a skateboard trick": "person doing sports",
    "woman skating": "person doing sports",
    "woman skiing": "person doing sports",
    "woman surfing": "person doing sports",
    "man teaching kids tennis": "person doing sports",
    "man ready with a bat with the umpire behind": "baseball player",
    "woman playing tennis": "tennis player",
    "old tennis player": "tennis player",

    # Water-related activities
    "man in a canoe": "person in watercraft",
    "man with kayaking equipment": "person in watercraft",
    "man holding surf board": "surfer",

    # Music/performance
    "man holding up a phone in front of a mariachi band": "music performance",
    "man in the air in front of a crowd": "performance",

    # Food/dining
    "a meal with a side of wine": "meal with drinks",
    "food tray": "served meal",
    "chicken sandwhich": "sandwich",
    "cupcakes": "dessert",
    "donuts": "dessert",
    "wedding cake": "celebration cake",
    "pizza dough": "raw food",

    # Animals
    "baby bear": "bear",
    "sheep dog guiding sheep": "working dog",
    "pigeons on a ledge": "birds",

    # Technology
    "laptop monitor and keyboard": "computer setup",
    "person using a phone in front of a laptop": "person using tech",
    "full system set up": "electronics setup",

    # Transportation
    "motorcycles parked together": "parked motorcycles",
    "train at the station": "train",
    "driving signs": "road signs",
    "traffic light with two cars below": "traffic scene",

    # Household items
    "bathtub and toilet seat": "bathroom fixtures",
    "retro toilet": "toilet",

    # People/groups
    "family in a bar": "group in bar",
    "man sitting with mother": "family",
    "two chefs in a kitchen": "kitchen staff",
    "two hands holding a white teddy": "hands holding object",
    "man with glasses and a white shirt": "man",
    "woman alone in aparrtment": "person indoors",

    # Misc
    "kite parade": "kite festival",
    "clock in a pub": "pub interior",
    "dishes on a table": "table setting",
    "salad spread": "food spread",
    "salad and spices": "food ingredients",
    "empty bench in a train station": "empty bench",
    "empty park bench": "empty bench",
    "man blowing air into a banana": "unusual action",
    "red umbrella in front of the ocean": "beach scene",
    "a fighter jet flying above the ground": "military aircraft"
}

def consolidate_categories(coco):
    """Merge similar categories to reduce class imbalance"""
    # Create new consolidated categories
    new_categories = []
    kept_categories = set()

    for cat in coco.dataset['categories']:
        if cat['name'] in CATEGORY_MAPPING:
            new_name = CATEGORY_MAPPING[cat['name']]
            if new_name not in kept_categories:
                new_categories.append({
                    'id': len(new_categories) + 1,
                    'name': new_name,
                    'supercategory': cat['supercategory']
                })
                kept_categories.add(new_name)
        else:
            if cat['name'] not in kept_categories:
                new_categories.append(cat)
                kept_categories.add(cat['name'])

    # Update annotations
    new_annotations = []
    for ann in coco.dataset['annotations']:
        cat_name = coco.loadCats(ann['category_id'])[0]['name']
        if cat_name in CATEGORY_MAPPING:
            new_cat_name = CATEGORY_MAPPING[cat_name]
            new_cat_id = [cat['id'] for cat in new_categories
                         if cat['name'] == new_cat_name][0]
            ann['category_id'] = new_cat_id
        new_annotations.append(ann)

    coco.dataset['categories'] = new_categories
    coco.dataset['annotations'] = new_annotations
    coco.createIndex()
    return coco

coco = consolidate_categories(coco)

# 4. Simplified Text Queries
def simplify_text_queries(category_names):
    """Clean up category names for better detection"""
    simplified = []
    for name in category_names:
        # Remove articles and prepositions
        words = [word for word in name.split()
                if word.lower() not in ['a', 'an', 'the', 'with', 'in', 'on', 'of']]
        # Take first 3 words max
        simplified.append(' '.join(words[:3]))
    return simplified

category_names = simplify_text_queries([cat['name'] for cat in coco.loadCats(coco.getCatIds())])

# 6. Enhanced Detection Pipeline
CONFIG = {
    'text_length': 16,
    'init_threshold': 0.01,  # Very low initial threshold
    'final_threshold': 0.3,   # Higher final confidence
    'min_size': 40,           # Only consider objects >40px
    'nms_iou': 0.4,           # Stricter overlap threshold
    'top_k': 3                # Max detections per image
}

def process_detections(coco, processor, model, images_dir, category_names):
    """Enhanced detection with better text handling"""
    filename_to_id = {img['extra']['name']: img['id'] for img in coco.loadImgs(coco.getImgIds())}
    detections = []

    # Simplify text queries further
    simple_queries = [q.split('(')[0].split(':')[0].lower() for q in category_names]

    for img_file in tqdm(os.listdir(images_dir), desc="Processing Images"):
        if not img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
            continue

        img_id = filename_to_id.get(img_file)
        if img_id is None:
            continue

        try:
            image = Image.open(os.path.join(images_dir, img_file)).convert("RGB")

            # Process with simplified queries
            inputs = processor(
                text=simple_queries,
                images=image,
                return_tensors="pt",
                padding=True,
                truncation=True
            )

            with torch.no_grad():
                outputs = model(**inputs)

            # Process results
            target_sizes = torch.tensor([image.size[::-1]])
            results = processor.post_process_grounded_object_detection(
                outputs,
                target_sizes=target_sizes,
                threshold=CONFIG['init_threshold']
            )[0]

            # Keep only top-k detections per image
            top_indices = results["scores"].topk(min(CONFIG['top_k'], len(results["scores"])))[1]

            for idx in top_indices:
                box = results["boxes"][idx]
                score = results["scores"][idx]
                label = results["labels"][idx]

                width = (box[2] - box[0]).item()
                height = (box[3] - box[1]).item()

                if (score.item() >= CONFIG['final_threshold'] and
                    width >= CONFIG['min_size'] and
                    height >= CONFIG['min_size']):

                    detections.append({
                        "image_id": img_id,
                        "category_id": coco.getCatIds()[label.item()],
                        "bbox": [box[0].item(), box[1].item(), width, height],
                        "score": score.item()
                    })

        except Exception as e:
            print(f"Error processing {img_file}: {str(e)}")
            continue

    return detections



# 8. Visualization
def visualize_top_detections(coco, detections, images_dir, num_samples=3):
    """Show best detections with annotations"""
    # Group by image and get highest score
    img_scores = {}
    for det in detections:
        if det['image_id'] not in img_scores:
            img_scores[det['image_id']] = det['score']
        else:
            img_scores[det['image_id']] = max(img_scores[det['image_id']], det['score'])

    # Sort images by best detection score
    sorted_img_ids = sorted(img_scores.keys(), key=lambda x: img_scores[x], reverse=True)

    # Visualize top images
    for img_id in sorted_img_ids[:num_samples]:
        img_info = coco.loadImgs(img_id)[0]
        img_path = os.path.join(images_dir, img_info['extra']['name'])

        try:
            image = Image.open(img_path)
            plt.figure(figsize=(12, 8))
            plt.imshow(image)

            # Show ground truth
            for ann in coco.loadAnns(coco.getAnnIds(imgIds=img_id)):
                box = ann['bbox']
                cat_name = coco.loadCats(ann['category_id'])[0]['name']
                plt.gca().add_patch(plt.Rectangle(
                    (box[0], box[1]), box[2], box[3],
                    fill=False, edgecolor='green', linewidth=2, linestyle='--'
                ))
                plt.text(box[0], box[1]-10, f"GT: {cat_name}",
                        color='white', bbox=dict(facecolor='green', alpha=0.7))

            # Show detections
            img_dets = [d for d in detections if d['image_id'] == img_id]
            for det in sorted(img_dets, key=lambda x: x['score'], reverse=True)[:3]:
                box = det['bbox']
                cat_name = coco.loadCats(det['category_id'])[0]['name']
                plt.gca().add_patch(plt.Rectangle(
                    (box[0], box[1]), box[2], box[3],
                    fill=False, edgecolor='red', linewidth=2
                ))
                plt.text(box[0], box[1]+box[3]+5, f"{cat_name} {det['score']:.2f}",
                        color='white', bbox=dict(facecolor='red', alpha=0.7))

            plt.title(f"Image ID: {img_id} (Best Score: {img_scores[img_id]:.2f})")
            plt.axis('off')
            plt.show()

        except Exception as e:
            print(f"Couldn't visualize image {img_id}: {str(e)}")

def evaluate_results(coco, detections):
    """Run evaluation with proper dimension handling"""
    # Save temporary results
    with open("temp_detections.json", "w") as f:
        json.dump(detections, f)

    # Load and evaluate
    coco_dt = coco.loadRes("temp_detections.json")
    coco_eval = COCOeval(coco, coco_dt, 'bbox')
    coco_eval.evaluate()
    coco_eval.accumulate()
    coco_eval.summarize()

    # Per-category analysis with proper array access
    print("\nPer-category performance:")
    for cat_id in coco.getCatIds():
        cat_name = coco.loadCats(cat_id)[0]['name']

        try:
            # Precision at IoU=0.5 (index 0)
            precisions = coco_eval.eval['precision'][0, :, cat_id, 0]
            ap = np.mean(precisions[precisions > -1])

            # Recall at maxDets=100 (last index)
            recalls = coco_eval.eval['recall'][0, :, cat_id, 0]
            ar = np.mean(recalls[recalls > -1])

            print(f"{cat_name}: AP={ap:.3f}, AR={ar:.3f}")
        except:
            print(f"{cat_name}: Could not calculate metrics")

if __name__ == "__main__":
    # 1. Process with new settings
    print("Starting detection pipeline...")
    detections = process_detections(coco, processor, model, images_dir, category_names)

    # 2. Apply NMS
    print("Applying non-maximum suppression...")
    final_detections = remove_overlapping_detections(detections, CONFIG['nms_iou'])

    # 3. Save results
    print("Saving results...")
    with open("optimized_detections.json", "w") as f:
        json.dump(final_detections, f, indent=2)

    # 4. Evaluate
    print("\nEvaluation metrics:")
    evaluate_results(coco, final_detections)

    # 5. Visualize
    print("\nVisualizing top detections...")
    visualize_top_detections(coco, final_detections, images_dir)

!pip install git+https://github.com/lvis-dataset/lvis-api.git

